---
title: "Chapter 2 - Key Methodological Concepts"
author: "Vanilton Paulo"
date: "2025-08-15"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup,echo=TRUE,message=FALSE}
# ─ Packages ───────────────────
# List of required packages
packages <- c(
  "refund",
  "tidyverse",
  "tidyfun",
  "gridExtra"
)

# Install missing packages
install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

# Install missing packages
invisible(lapply(packages, install_if_missing))



#Load packages
library(refund)      
library(tidyverse)   
library(tidyfun)     
library(gridExtra)

#For reproducible results
set.seed(53615)
```

```{r helpers,echo=TRUE}
# ─────── helpers ───────────────────
#Converts calendar dates to numeric values 
#The tfd requires numeric values and not as dates values.
num_grid <- function(dates, ref = min(dates)) as.numeric(dates - ref)
reference_date <- as.Date("2020-01-01")
```

## Weekly all-cause mortality data

This section demonstrates how to perform **functional principal
component analysis (fPCA)** on the cumulative weekly all-cause mortality
data for the 52 U.S. states and territories.

```{r load-data,echo=TRUE, results='hide'}
# ──── Overview ───────────────────
data("COVID19", package = "refund") 
names(COVID19)
##  [1] "US_weekly_mort"                      "US_weekly_mort_dates"               
##  [3] "US_weekly_mort_CV19"                 "US_weekly_mort_CV19_dates"          
##  [5] "US_weekly_excess_mort_2020"          "US_weekly_excess_mort_2020_dates"   
##  [7] "US_states_names"                     "US_states_population"               
##  [9] "States_excess_mortality"             "States_excess_mortality_per_million"
## [11] "States_CV19_mortality"               "States_CV19_mortality_per_million"

# ──────── Data ─────────────────────────────────
new_states <- COVID19$US_states_names
current_date <- COVID19$US_weekly_excess_mort_2020_dates
Wd <- COVID19$States_excess_mortality_per_million  
```

```{r , include=TRUE, fig.width= 10}
# ── Data Transformation ───────────────────
#Calculate the cumulative excess mortality per million for each U.S. state and territory.
cum_mat <- t(apply(Wd, 1, cumsum))

# ──────── Turn into tfd ──────────────────────
states_cum_tf <- tfd(
  cum_mat,
  arg = num_grid(current_date),
  id  = new_states,
  var = "cum_excess"
)
# tfd[52] on (0,357) based on 52 evaluations each
# interpolation by tf_approx_linear 
# [1]: ( 0, 0.8);( 7, 8.3);(14,-6.9); ...
# [2]: ( 0,  -5);( 7,  19);(14,  26); ...
# [3]: ( 0,  -7);( 7,  -1);(14,   5); ...
# [4]: ( 0,  -7);( 7,  21);(14,  20); ...
# [5]: ( 0,   5);( 7,   7);(14,  12); ...
#     [....]   (47 not shown)
# ── grand‐mean & centered curves ─────────────────────────────────────────
#Average curve across all states
mean_curve <- mean(states_cum_tf)

#Subtracts the mean curve from each state's curve, so that we can analyze deviations from the national pattern.
centered_states_cum_tf <- scale(states_cum_tf, scale = FALSE)
# ── Plot ────────────────
#Visualize the data for each state and territory (gray solid lines) and the mean (dark red solid line).
ggplot() +
  geom_spaghetti(aes(y = states_cum_tf),
                 colour = "grey20", alpha = 0.15) +
  geom_spaghetti(aes(y = mean_curve),
                 colour = "darkred", linewidth = 1.4) +
  scale_x_continuous(
    name = "Weeks starting January 2020",
    breaks = as.numeric(seq(reference_date, as.Date("2021-01-01"), by = "3 months") - reference_date),
    labels = format(seq(reference_date, as.Date("2021-01-01"), by = "3 months"), "%b %Y")
  ) +
  labs(
    x = "Weeks starting January 2020",
    y = "Cumulative excess deaths per million"
  ) +
  theme_classic(base_size = 13)
```
FIGURE 2.1: This plot displays the cumulative weekly excess deaths per million for all 52 U.S. states and territories (gray lines). The bold dark red line represents the mean cumulative excess mortality across all states.

#### Display the de-meaned data

To illustrate the impact of removing the mean from the original data, we
plot the resulting curves. The following five states are emphasized: New
Jersey (green), Louisiana (red), California (plum), Maryland (blue), and
Texas (salmon).

```{r , include=TRUE, fig.width= 10}
centered_cum_states_tf_v2 <- tfd(
  t(apply(centered_states_cum_tf, 1, identity)),
  arg = num_grid(current_date),
  id = new_states
)
# tfd[52] on (0,357) based on 52 evaluations each
# interpolation by tf_approx_linear 
# [1]: ( 0,  -5);( 7,  -3);(14, -22); ...
# [2]: ( 0, -11);( 7,   8);(14,  11); ...
# [3]: ( 0, -13);( 7, -12);(14, -10); ...
# [4]: ( 0, -12);( 7,  10);(14,   5); ...
# [5]: ( 0,-0.6);( 7,-3.7);(14,-3.2); ...
#     [....]   (47 not shown)
# ── Plot ────────────────

ggplot() +
  geom_spaghetti(aes(y = centered_cum_states_tf_v2),
                 colour = "grey60", alpha = 0.2, linewidth = 0.5) +
  geom_spaghetti(aes(y = centered_cum_states_tf_v2[which(new_states == "New Jersey")]),
                 colour = "darkseagreen3", linewidth = 1.2, alpha = 3) +
  geom_spaghetti(aes(y = centered_cum_states_tf_v2[which(new_states == "Louisiana")]),
                 colour = "red", linewidth = 1.2, alpha = 3) +
  geom_spaghetti(aes(y = centered_cum_states_tf_v2[which(new_states == "California")]),
                 colour = "plum3", linewidth = 1.2, alpha = 3) +
  geom_spaghetti(aes(y = centered_cum_states_tf_v2[which(new_states == "Maryland")]),
                 colour = "deepskyblue4", linewidth = 1.2, alpha = 3) +
  geom_spaghetti(aes(y = centered_cum_states_tf_v2[which(new_states == "Texas")]),
                 colour = "salmon", linewidth = 1.2, alpha = 3) +
  scale_x_continuous(
    name = "Weeks starting January 2020",
    breaks = as.numeric(seq(reference_date, as.Date("2021-01-01"), by = "3 months") - reference_date),
    labels = format(seq(reference_date, as.Date("2021-01-01"), by = "3 months"), "%b %Y")
  ) +
  labs(
    x = "Weeks starting January 2020",
    y = "US states centered cumulative excess deaths/million"
  ) +
  theme_classic(base_size = 13)
```


FIGURE 2.2: This plot shows the centered cumulative excess deaths per million, which are the deviations from the mean national trend for each state. The gray lines represent all states, while five specific states are highlighted to emphasize their individual trends: New Jersey (dark green), Louisiana (red), California (plum), Maryland (deepskyblue), and Texas (salmon).


Singular Value Decomposition (`svd`)

For a data matrix $X \in \mathbb{R}^{n\times p}$, $$
X \;=\; U\,D\,V^\top .
$$

Where**:**

-   $U \in \mathbb{R}^{n\times n}$ — orthogonal; columns are the left
    singular vectors.

-   $D \in \mathbb{R}^{n\times p}$ — diagonal/rectangular with
    non-negative diagonal entries $d_1 \ge \cdots \ge d_r > 0$ (the
    singular values; $r=\mathrm{rank}(X)$).

-   $V \in \mathbb{R}^{p\times p}$ — orthogonal; columns are the right
    singular vectors.

Why it’s useful:

-   Best low-rank approximation**:** for any $k<r$,
    $X_k=U_k D_k V_k^\top$ minimizes the reconstruction error
    (Frobenius/spectral norms).

-   Link to PCA: if columns of $X$ are centered, $V_r$ are principal
    directions and $d_i^2$ are variances explained; $U_r D_r$ are the PC
    scores.

`tfb_fpc` performs Functional Principal Component Analysis (FPCA) and
returns a functional data object in FPC basis representation. It's
specifically designed for functional data analysis.

Default Method: `fpc_wsvd`

The default method, `fpc_wsvd`, uses a weighted SVD tailored to
functional data.

Key features:

-   Weighted orthogonality**.** Using trapezoidal quadrature weights
    $\Delta_i$ at grid points $t_i$, the eigenfunctions $\phi_j$ are
    orthonormal with respect to the $L^2$ inner product: $$
    \int_T \phi_j(t)^2\,dt \;\approx\; \sum_i \Delta_i\,\phi_j(t_i)^2 \;=\; 1,
    $$ rather than simple Euclidean orthogonality
    $\sum_i \phi_j(t_i)^2 = 1$.

-   Functional orthogonality**.** For $j \neq k$, $$
    \int_T \phi_j(t)\,\phi_k(t)\,dt \;=\; 0 
    \;\approx\; \sum_i \Delta_i\,\phi_j(t_i)\,\phi_k(t_i) \;=\; 0.
    $$

Why `svd` and `tfb_fpc` are they equivalent?

**Mathematical Equivalence**

Both methods solve the same underlying problem:

For centered data matrix $\mathbf{W}$ (mean removed):

1.  SVD approach: $\mathbf{W} = \mathbf{U} \mathbf{D} \mathbf{V}^T$
    -   $\mathbf{V}$ contains the eigenfunctions (right singular
        vectors)
2.  FPCA approach: Find eigenfunctions $\phi_k$ that maximize
    $$\text{Var}\left(\int X(t) \phi_k(t) dt\right)$$
    -   These $\phi_k$ are the same as columns of $\mathbf{V}$ from SVD

Furthermore

-   SVD's right singular vectors ($\mathbf{V}$) = FPCA's eigenfunctions
    ($\phi$)
-   SVD's left singular vectors ($\mathbf{U}$) × singular values
    ($\mathbf{D}$) = FPCA scores
-   Both decompose the same covariance structure

------------------------------------------------------------------------

While the book's solution uses Singular Value Decomposition (`svd`), a
more specialized approach for functional data involves using Functional
Principal Component Analysis (fPCA). The `tfb_fpc` function is
specifically designed for this purpose, as it treats each state's
mortality data as a continuous curve, or a function, rather than a
simple collection of data points.

#### Display the second and third eigenfunctions.

The `tfb_fpc` function is then applied to the centered data to perform
fPCA. The argument `pve = 0.99` instructs the function to select enough
principal components to explain 99% of the total variance in the data.

```{r , include=TRUE, fig.width = 9}
# ───── Display the second and third eigenfunctions. ─────────────────────────────

# In place of the scale function, the mean curve was subtracted. 
centered_states_cum_tf <- states_cum_tf - mean_curve

# FPCA
covid_pc <- tfb_fpc(centered_states_cum_tf, pve = 0.99)
# tfb[52] on (0,357) in basis representation:
#  using  4 FPCs 
# [1]: ( 0,-1.2);( 7,-0.4);(14,-1.6); ...
# [2]: ( 0,  -5);( 7,  -5);(14,  -8); ...
# [3]: ( 0,  -4);( 7,  -3);(14,  -8); ...
# [4]: ( 0,-0.6);( 7,-0.7);(14,-1.5); ...
# [5]: ( 0,  -4);( 7,  -3);(14,  -5); ...
#     [....]   (47 not shown)
```

The `tf_basis` function is used to extract the eigenfunctions, which are
the fundamental shapes of variation underlying the data. They are the
functional equivalent of eigenvectors in standard PCA.

```{r , include=TRUE, fig.width = 9}
# Extract the eigenfunctions from the fPCA result
basis_functions <- tf_basis(covid_pc, as_tfd = TRUE)
# tfd[5] on (0,357) based on 52 evaluations each
# interpolation by tf_approx_linear 
# [1]: ( 0, 5e-16);( 7, 1e-16);(14, 1e-15); ...
# [2]: ( 0, 3e-04);( 7, 3e-04);(14, 3e-04); ...
# [3]: ( 0,-6e-04);( 7,-9e-04);(14,-2e-03); ...
# [4]: ( 0,-0.003);( 7,-0.001);(14,-0.003); ...
# [5]: ( 0,-0.004);( 7,-0.010);(14,-0.023); ...



ggplot() +
  geom_spaghetti(aes(y = basis_functions[2]),
                 colour = "coral", alpha = 2) +
  geom_spaghetti(aes(y = basis_functions[3]),
                 colour = "coral4", alpha = 2) +
  scale_x_continuous(
    name = "Weeks starting January 2020",
    breaks = as.numeric(seq(reference_date, as.Date("2021-01-01"), by = "3 months") - reference_date),
    labels = format(seq(reference_date, as.Date("2021-01-01"), by = "3 months"), "%b %Y")
  ) +
  labs(
    x = "Weeks starting January 2020",
    y = "Eigenfunctions"
  ) +
  theme_classic(base_size = 13)
```

FIGURE 2.3: This plot displays the second and third eigenfunctions derived from the Functional Principal Component Analysis (FPCA). These curves represent the fundamental shapes of variation that underlie the data after accounting for the primary mode of variation captured by the first eigenfunction

#### Reconstruction of the original data

```{r , include=TRUE, fig.width=9}
# ──────────────────── Compare the real and reconstructed data ──────────────────────

#Uses functional principal components
svd_tfb <- tfb_fpc(states_cum_tf, pve = .9)
# tfb[52] on (0,357) in basis representation:
#  using  2 FPCs 
# [1]: ( 0, 9);( 7,15);(14,22); ...
# [2]: ( 0, 2);( 7, 7);(14,10); ...
# [3]: ( 0, 9);( 7,15);(14,22); ...
# [4]: ( 0, 7);( 7,14);(14,22); ...
# [5]: ( 0, 4);( 7,10);(14,13); ...
#     [....]   (47 not shown)
```

#### Compare the real and reconstructed data

```{r , include=TRUE, fig.width=9}
emphasize <- c("New Jersey", "Louisiana", "California", "Maryland", "Texas")
emph_cols <- c("plum3","red", "deepskyblue4","darkseagreen3", "salmon")

# Create comparison data frame
comp_df <- tibble(
  state = new_states,
  original_curve = states_cum_tf,
  # tfb_fpc reconstruction
  recon_tfb = svd_tfb            
) %>%
  filter(state %in% emphasize)
## # A tibble: 5 × 3
##   state                         original_curve                         recon_tfb
##   <chr>                              <tfd_reg>                         <tfb_fpc>
## 1 California [1]: ( 0, 5);( 7, 7);(14,12); ... [1]: ( 0, 4);( 7,10);(14,13); ...
## 2 Louisiana  [2]: ( 0, 6);( 7,16);(14,11); ... [2]: ( 0,11);( 7,16);(14,23); ...
## 3 Maryland   [3]: ( 0, 1);( 7, 7);(14, 7); ... [3]: ( 0, 6);( 7,11);(14,14); ...
## 4 New Jersey [4]: ( 0,11);( 7,22);(14,20); ... [4]: ( 0,11);( 7,14);(14,15); ...
## 5 Texas      [5]: ( 0, 1);( 7,11);(14,14); ... [5]: ( 0, 7);( 7,13);(14,19); ...
# ───── Plot ────────────────────────────────────

#Original vs tfb_fpc 
 ggplot(comp_df) +
  geom_spaghetti(aes(y = original_curve, color = state),
                 linewidth = 2, alpha = 0.8) +
  geom_spaghetti(aes(y = recon_tfb, color = state),
                 linewidth = 2, linetype = "dashed", alpha = 0.8) +
  scale_color_manual(values = emph_cols) +
  scale_x_continuous(
    name = "Weeks starting January 2020",
    breaks = as.numeric(seq(reference_date, as.Date("2021-01-01"), by = "3 months") - reference_date),
    labels = format(seq(reference_date, as.Date("2021-01-01"), by = "3 months"), "%b %Y")
  ) +
  labs(
    x = "Weeks starting January 2020",
    y = "Cumulative excess deaths per million",
    color = NULL
  ) +
  theme_classic(base_size = 14)

```
FIGURE 2.4: This plot compares the All-cause excess mortality (solid lines) with the curves reconstructed using a limited number of functional principal components (dashed lines). The reconstruction uses the first few components that explain 90% of the variance, demonstrating the effectiveness of FPCA in approximating the original data with fewer dimensions.